{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIPFUXukA7qWWAldO+hJx6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armandossrecife/piloto/blob/main/testes_acesso_jira.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DependÃªncias\n",
        "\n"
      ],
      "metadata": {
        "id": "IMgOk-3xkAR6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u17XzSl7_C2P"
      },
      "outputs": [],
      "source": [
        "!pip install jira > install_jira.log\n",
        "!pip install sqlite3 > install_sqlite.log\n",
        "!sudo apt install sqlite3 > install_sqlite3.log"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apoio (Classes)"
      ],
      "metadata": {
        "id": "nboRDkedkFFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jira import JIRA\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import tqdm\n",
        "import sqlite3\n",
        "import subprocess\n",
        "import sqlite3\n",
        "\n",
        "JIRA_SERVER = 'https://issues.apache.org/jira'\n",
        "DATABASE_NAME = \"issues_db.db\"\n",
        "\n",
        "os.environ['DATABASE_NAME'] = DATABASE_NAME\n",
        "# Credentials\n",
        "os.environ['USERNAME'] = 'armandossrecife'\n",
        "os.environ['PASSWORD'] = 'sky1979#ce'\n",
        "username = os.environ.get('USERNAME')\n",
        "password = os.environ.get('PASSWORD')\n",
        "\n",
        "class JiraIssue:\n",
        "  def __init__(self, key, summary, issue_type, status, priority, description, comments):\n",
        "    self.key = key\n",
        "    self.summary = summary\n",
        "    self.issue_type = issue_type\n",
        "    self.status = status\n",
        "    self.priority = priority\n",
        "    self.description = description\n",
        "    self.comments = comments\n",
        "\n",
        "  def get_comments(self) -> dict:\n",
        "    return self.comments\n",
        "\n",
        "  def __str__(self):\n",
        "    return (f'Key: {self.key}, Summary: {self.summary}, Type: {self.issue_type}, Status: {self.status}')\n",
        "\n",
        "class JiraIssues:\n",
        "  def __init__(self,project, issues):\n",
        "    self.project = project\n",
        "    self.issues = issues\n",
        "\n",
        "  def add_issue(self, issue):\n",
        "    self.issues.append(issue)\n",
        "\n",
        "  def get_issues(self) -> list:\n",
        "    return self.issues\n",
        "\n",
        "  def update_issues(self, issues):\n",
        "    self.issues = issues\n",
        "\n",
        "  def __str__(self):\n",
        "    str_issues = \"\"\n",
        "    for issue in self.get_issues():\n",
        "      str_issues = str_issues + str(issue)\n",
        "      str_issues = str_issues + ', '\n",
        "    str_issues = '[' + str_issues + ']'\n",
        "    return (f'Project: {self.project}, Qdt of issues: {len(self.issues)}, Issues: {str_issues}')\n",
        "\n",
        "# Classe de utilidades para manipular o servidor Jira\n",
        "class JiraUtils:\n",
        "  def __init__(self, project, jira_instance):\n",
        "    self.project = project\n",
        "    self.jira_jira_instance = jira_instance\n",
        "\n",
        "  def generate_intervals_between_dates(self, date1: tuple, date2: tuple, distance=120) -> list:\n",
        "    start_date = datetime(date1[0], date1[1], date1[2])\n",
        "    end_date = datetime(date2[0], date2[1], date2[2])\n",
        "    interval_days = distance\n",
        "    # Initialize a list to store the intervals\n",
        "    intervals = []\n",
        "    # Initialize the current date as the start date\n",
        "    current_date = start_date\n",
        "    # Loop to generate intervals until the current date is less than or equal to the end date\n",
        "    while current_date < end_date:\n",
        "        interval = (current_date, current_date + timedelta(days=interval_days - 1))\n",
        "        intervals.append(interval)\n",
        "        current_date += timedelta(days=interval_days)\n",
        "    return intervals\n",
        "\n",
        "  def convert_interval_dates(self, dates: list) -> list:\n",
        "    list_interval_dates = []\n",
        "    for each in dates:\n",
        "      date1 = each[0]\n",
        "      # Convert the date to a string in the format \"YYYY/MM/DD\".\n",
        "      str_date1 = date1.strftime(\"%Y/%m/%d\")\n",
        "      date2 = each[1]\n",
        "      str_date2 = date2.strftime(\"%Y/%m/%d\")\n",
        "      elemento = str_date1, str_date2\n",
        "      list_interval_dates.append(elemento)\n",
        "    return list_interval_dates\n",
        "\n",
        "  def generate_list_of_sentences(self, dates: list) -> list:\n",
        "    lista_sentencas = []\n",
        "    for each in dates:\n",
        "      str_date1 = each[0].strftime(\"%Y/%m/%d\")\n",
        "      str_date2 = each[1].strftime(\"%Y/%m/%d\")\n",
        "      sentenca = f'project={self.project.upper()} and created>=\"{str_date1}\" and created<=\"{str_date2}\"'\n",
        "      lista_sentencas.append(sentenca)\n",
        "    return lista_sentencas\n",
        "\n",
        "  def get_list_of_block_issues_by_dates(self,date1, date2, distance=120) -> list:\n",
        "    print('Aguarde...')\n",
        "    t1 = datetime.now()\n",
        "    list_of_dates = self.generate_intervals_between_dates(date1,date2,distance)\n",
        "    lista_sentencas = self.generate_list_of_sentences(list_of_dates)\n",
        "    lista_bloco_issues_by_date = []\n",
        "    total_items = len(lista_sentencas)\n",
        "    i = 0\n",
        "    iterable_lista_sentencas = tqdm.tqdm(lista_sentencas, total=total_items)\n",
        "    for each in iterable_lista_sentencas:\n",
        "      issues_by_date_temp = self.jira_jira_instance.search_issues(each,maxResults=1000)\n",
        "      print(f'Range: {each}, qtd issues: {len(issues_by_date_temp)}')\n",
        "      lista_bloco_issues_by_date.append(issues_by_date_temp)\n",
        "      percentage = (i + 1) / total_items * 100\n",
        "      iterable_lista_sentencas.set_description(f\"Progress Message Analysis\")\n",
        "    i += 1\n",
        "    t2 = datetime.now()\n",
        "    print(t2)\n",
        "    print(f'Tempo da consulta: {t2-t1}')\n",
        "    return lista_bloco_issues_by_date\n",
        "\n",
        "  def concatenate_block_of_issues(self,block_of_issues):\n",
        "    concatenated_list = [item for sublist in block_of_issues for item in sublist]\n",
        "    print(f'Total de issues recuperados: {len(concatenated_list)}')\n",
        "    return concatenated_list\n",
        "\n",
        "class IssuesDatabase:\n",
        "    def __init__(self, database_name):\n",
        "        self.database_name = database_name\n",
        "        self.create_tables()\n",
        "\n",
        "    def create_tables(self):\n",
        "        self.conn = sqlite3.connect(self.database_name)\n",
        "        self.cursor = self.conn.cursor()\n",
        "\n",
        "        self.cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS Issues (\n",
        "                id INTEGER PRIMARY KEY,\n",
        "                project TEXT,\n",
        "                key TEXT,\n",
        "                summary TEXT,\n",
        "                issue_type TEXT,\n",
        "                status TEXT,\n",
        "                priority TEXT,\n",
        "                description TEXT\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        self.cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS Comments (\n",
        "                id INTEGER PRIMARY KEY,\n",
        "                key TEXT,\n",
        "                comment TEXT\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        self.conn.commit()\n",
        "\n",
        "    def insert_in_table_issues(self, project, key, summary, issue_type, status, priority, description):\n",
        "        values = (None, project, key, summary, issue_type, status, priority, description)\n",
        "        self.cursor.execute('''\n",
        "            INSERT INTO Issues\n",
        "            (id, project, key, summary, issue_type, status, priority, description)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        ''', values)\n",
        "\n",
        "        self.conn.commit()\n",
        "\n",
        "    def insert_in_table_comments(self, key, comment):\n",
        "        values = (None, key, comment)\n",
        "        self.cursor.execute('''\n",
        "            INSERT INTO Comments\n",
        "            (id, key, comment)\n",
        "            VALUES (?, ?, ?)\n",
        "        ''', values)\n",
        "\n",
        "        self.conn.commit()\n",
        "\n",
        "    def show_content(self, table):\n",
        "        query = f\"SELECT * FROM {table}\"\n",
        "        self.cursor.execute(query)\n",
        "\n",
        "        rows = self.cursor.fetchall()\n",
        "        for row in rows:\n",
        "            print(row)\n",
        "\n",
        "    def show_n_lines(self, table, n):\n",
        "        query = f\"SELECT * FROM {table}\"\n",
        "        self.cursor.execute(query)\n",
        "\n",
        "        rows = self.cursor.fetchall()\n",
        "\n",
        "        for i, row in enumerate(rows):\n",
        "            print(row)\n",
        "            if i == n:\n",
        "              break\n",
        "\n",
        "    def close_connection(self):\n",
        "        self.conn.close()"
      ],
      "metadata": {
        "id": "UZ1K49vm_EgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cria uma instÃ¢ncia para acessar o servidor Jira"
      ],
      "metadata": {
        "id": "VdHP9d8WkMUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a JIRA client instance\n",
        "jira_instance = JIRA(server=JIRA_SERVER, basic_auth=(username, password))\n",
        "\n",
        "jira_utilidades = JiraUtils(project='CASSANDRA', jira_instance=jira_instance)"
      ],
      "metadata": {
        "id": "KNFTgsRx-Cei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coleta os issues do projeto Cassandra"
      ],
      "metadata": {
        "id": "XQ4e2m-ukWPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cassandra_block_issues_by_date_2009_2023 = jira_utilidades.get_list_of_block_issues_by_dates(date1=(2009,3,2), date2=(2023,9,12), distance=120)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFxoOmZNOaMi",
        "outputId": "770db540-12d3-43a7-c1e1-27e66db2b8d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aguarde...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:   2%|â         | 1/45 [00:02<02:05,  2.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2009/03/02\" and created<=\"2009/06/29\", qtd issues: 263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:   4%|â         | 2/45 [00:06<02:15,  3.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2009/06/30\" and created<=\"2009/10/27\", qtd issues: 251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:   7%|â         | 3/45 [00:09<02:22,  3.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2009/10/28\" and created<=\"2010/02/24\", qtd issues: 310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:   9%|â         | 4/45 [00:14<02:38,  3.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2010/02/25\" and created<=\"2010/06/24\", qtd issues: 388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  11%|â         | 5/45 [00:19<02:54,  4.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2010/06/25\" and created<=\"2010/10/22\", qtd issues: 419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  13%|ââ        | 6/45 [00:26<03:19,  5.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2010/10/23\" and created<=\"2011/02/19\", qtd issues: 545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  16%|ââ        | 7/45 [00:33<03:42,  5.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2011/02/20\" and created<=\"2011/06/19\", qtd issues: 593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  18%|ââ        | 8/45 [00:40<03:42,  6.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2011/06/20\" and created<=\"2011/10/17\", qtd issues: 573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  20%|ââ        | 9/45 [00:46<03:42,  6.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2011/10/18\" and created<=\"2012/02/14\", qtd issues: 532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  22%|âââ       | 10/45 [00:51<03:21,  5.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2012/02/15\" and created<=\"2012/06/13\", qtd issues: 423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  24%|âââ       | 11/45 [00:57<03:15,  5.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2012/06/14\" and created<=\"2012/10/11\", qtd issues: 441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  27%|âââ       | 12/45 [01:02<03:05,  5.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2012/10/12\" and created<=\"2013/02/08\", qtd issues: 433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  29%|âââ       | 13/45 [01:06<02:47,  5.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2013/02/09\" and created<=\"2013/06/08\", qtd issues: 386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  31%|âââ       | 14/45 [01:13<02:58,  5.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2013/06/09\" and created<=\"2013/10/06\", qtd issues: 522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  33%|ââââ      | 15/45 [01:19<02:54,  5.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2013/10/07\" and created<=\"2014/02/03\", qtd issues: 492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  36%|ââââ      | 16/45 [01:29<03:21,  6.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2014/02/04\" and created<=\"2014/06/03\", qtd issues: 690\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  38%|ââââ      | 17/45 [01:38<03:34,  7.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2014/06/04\" and created<=\"2014/10/01\", qtd issues: 680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  40%|ââââ      | 18/45 [01:46<03:31,  7.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2014/10/02\" and created<=\"2015/01/29\", qtd issues: 655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  42%|âââââ     | 19/45 [01:57<03:44,  8.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2015/01/30\" and created<=\"2015/05/29\", qtd issues: 802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  44%|âââââ     | 20/45 [02:07<03:48,  9.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2015/05/30\" and created<=\"2015/09/26\", qtd issues: 875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  47%|âââââ     | 21/45 [02:15<03:28,  8.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2015/09/27\" and created<=\"2016/01/24\", qtd issues: 659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  49%|âââââ     | 22/45 [02:24<03:21,  8.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2016/01/25\" and created<=\"2016/05/23\", qtd issues: 769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  51%|âââââ     | 23/45 [02:34<03:20,  9.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2016/05/24\" and created<=\"2016/09/20\", qtd issues: 787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  53%|ââââââ    | 24/45 [02:39<02:47,  7.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2016/09/21\" and created<=\"2017/01/18\", qtd issues: 453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  56%|ââââââ    | 25/45 [02:44<02:19,  6.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2017/01/19\" and created<=\"2017/05/18\", qtd issues: 403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  58%|ââââââ    | 26/45 [02:47<01:53,  5.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2017/05/19\" and created<=\"2017/09/15\", qtd issues: 338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  60%|ââââââ    | 27/45 [02:51<01:33,  5.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2017/09/16\" and created<=\"2018/01/13\", qtd issues: 282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  62%|âââââââ   | 28/45 [02:54<01:17,  4.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2018/01/14\" and created<=\"2018/05/13\", qtd issues: 277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  64%|âââââââ   | 29/45 [02:57<01:05,  4.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2018/05/14\" and created<=\"2018/09/10\", qtd issues: 265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  67%|âââââââ   | 30/45 [03:00<00:56,  3.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2018/09/11\" and created<=\"2019/01/08\", qtd issues: 250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  69%|âââââââ   | 31/45 [03:01<00:43,  3.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2019/01/09\" and created<=\"2019/05/08\", qtd issues: 143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  71%|âââââââ   | 32/45 [03:04<00:38,  2.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2019/05/09\" and created<=\"2019/09/05\", qtd issues: 177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  73%|ââââââââ  | 33/45 [03:06<00:31,  2.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2019/09/06\" and created<=\"2020/01/03\", qtd issues: 164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  76%|ââââââââ  | 34/45 [03:11<00:37,  3.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2020/01/04\" and created<=\"2020/05/02\", qtd issues: 306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  78%|ââââââââ  | 35/45 [03:15<00:35,  3.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2020/05/03\" and created<=\"2020/08/30\", qtd issues: 300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  80%|ââââââââ  | 36/45 [03:18<00:31,  3.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2020/08/31\" and created<=\"2020/12/28\", qtd issues: 284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  82%|âââââââââ | 37/45 [03:21<00:26,  3.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2020/12/29\" and created<=\"2021/04/27\", qtd issues: 262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  84%|âââââââââ | 38/45 [03:24<00:22,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2021/04/28\" and created<=\"2021/08/25\", qtd issues: 244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  87%|âââââââââ | 39/45 [03:28<00:20,  3.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2021/08/26\" and created<=\"2021/12/23\", qtd issues: 342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  89%|âââââââââ | 40/45 [03:32<00:18,  3.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2021/12/24\" and created<=\"2022/04/22\", qtd issues: 340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  91%|âââââââââ | 41/45 [03:35<00:13,  3.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2022/04/23\" and created<=\"2022/08/20\", qtd issues: 267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  93%|ââââââââââ| 42/45 [03:39<00:10,  3.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2022/08/21\" and created<=\"2022/12/18\", qtd issues: 273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  96%|ââââââââââ| 43/45 [03:43<00:07,  3.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2022/12/19\" and created<=\"2023/04/17\", qtd issues: 328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  98%|ââââââââââ| 44/45 [03:46<00:03,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2023/04/18\" and created<=\"2023/08/15\", qtd issues: 298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis: 100%|ââââââââââ| 45/45 [03:47<00:00,  5.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=CASSANDRA and created>=\"2023/08/16\" and created<=\"2023/12/13\", qtd issues: 95\n",
            "2023-09-16 17:13:42.408057\n",
            "Tempo da consulta: 0:03:47.784661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cassandra_issues_from_2009_to_2023 = jira_utilidades.concatenate_block_of_issues(block_of_issues=cassandra_block_issues_by_date_2009_2023)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70PqEr58Yhra",
        "outputId": "a1495a00-cff0-4d9d-fc8b-aa5ebf365c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de issues recuperados: 18579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Popula um manipulador de Issues do Jira"
      ],
      "metadata": {
        "id": "Z0OOrrmnka8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cassandra_issues = JiraIssues(project='Cassandra', issues=[])\n",
        "\n",
        "print('Popula issues do Cassandra')\n",
        "for i,issue in enumerate(cassandra_issues_from_2009_to_2023):\n",
        "  dict_comments_issue = {}\n",
        "  list_comments = []\n",
        "  for comment in issue.fields.comment.comments:\n",
        "    elemento = (comment.id, comment.body)\n",
        "    list_comments.append(elemento)\n",
        "  dict_comments_issue[issue.key] = list_comments\n",
        "  # Cria um JiraIssue\n",
        "  jira_issue = JiraIssue(key=issue.key, summary=issue.fields.summary, status=issue.fields.status, issue_type=issue.fields.issuetype,priority=issue.fields.priority,description=issue.fields.description, comments=dict_comments_issue)\n",
        "  cassandra_issues.add_issue(jira_issue)\n",
        "print(f'{len(cassandra_issues.get_issues())} issues populados com sucesso!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCnAMCBJdgrC",
        "outputId": "710db87c-36fd-4ae1-868e-d479ce377241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Popula issues do Cassandra\n",
            "18579 issues populados com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Mostra o conteÃºdo dos 10 primeiros issues...')\n",
        "for i,each in enumerate(cassandra_issues.get_issues()):\n",
        "  print(i+1, each)\n",
        "  print(f'Description: {each.description}')\n",
        "  for k,v in each.get_comments().items():\n",
        "    print(k)\n",
        "    print(f'Comments: {len(v)}')\n",
        "    for j in v:\n",
        "      print(j)\n",
        "  if i==10:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41WfNFW5f1P9",
        "outputId": "c6f89708-3bda-437d-d0cd-7e0449dcaa79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mostra o conteÃºdo dos 10 primeiros issues...\n",
            "1 Key: CASSANDRA-263, Summary: get_slice needs to support desc from last column, Type: Bug, Status: Resolved\n",
            "Bug Resolved Normal\n",
            "Description: At the moment there's no way to ask for a slice starting with the last column and going desc\n",
            "CASSANDRA-263\n",
            "Comments: 7\n",
            "('12724762', \"Is the problem that you don't know what the last column is? Normally, an application should be able to insert a fake column that's always larger than any real columns. Do you think that's good enough?\")\n",
            "('12724766', 'Yes.\\n\\nI was thinking that we could have the \"CF\" format (w/ no \":\") stand for \"start w/ first if asc or last if desc.\"  Reading the last from the column index should be reasonably efficient, right?')\n",
            "('12725251', 'Using the column index to start with the last column group is possible and is efficient. I am just wondering if it is a good idea to give empty starting column different meanings depending on the ordering.')\n",
            "('12725254', \"I'm open to alternative APIs, but that one makes sense to me and doesn't require adding extra parameters :)\")\n",
            "('12737354', \"Attach a fix. If startColumn is empty and isAscending is false, assume that we scan from the largest column in descending order.\\n\\nAlso fix a bug in SliceFilterQuery.filterSuperColumn() that didn't handle descending order properly.\\n\")\n",
            "('12737550', 'committed, thanks!')\n",
            "('12737872', 'Integrated in Cassandra #154 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/154/])\\n    allow start of [] to mean \"start with the largest value\" when ascending=false.  patch by Jun Rao; reviewed by jbellis for \\n')\n",
            "2 Key: CASSANDRA-262, Summary: get_slice needs to allow returning all columns, Type: Bug, Status: Resolved\n",
            "Bug Resolved Normal\n",
            "Description: Right now get_slice requires you to enter a 'large' value, -1 used to indicate 'all columns'. We should allow this.\n",
            "CASSANDRA-262\n",
            "Comments: 8\n",
            "('12726114', 'How about if we add a configuration parameter for \"maximum columns to slice at once,\" that defaults to something high enough (1000?) that you really ought to page if you need more than that anyway, and low enough that it\\'s in no danger of crashing your server from OOM?')\n",
            "('12726117', '+1. I am okay with this.')\n",
            "('12726121', \"Eric, since you're already in the slice code for CASSANDRA-263, do you think you could take a stab at this too?\")\n",
            "('12726248', 'Actually, as with many of my \"brilliant\" ideas, this one looks worse given a little more time. :)\\n\\nI don\\'t see what purpose this would serve over the client just sending 1000 for the count, other than obfuscation.')\n",
            "('12727167', \"For what it's worth, I'm having the Ruby client currently default to limit = 100...I couldn't really care less what the server thinks the limit is. I expect other clients would be similar.\\n\")\n",
            "('12727192', \"yes, that's the right idea.\\n\\nI set the default for all the count parameters to 100 in cassandra.thrift.  I don't know of any languages for which thrift actually generates working defaults but it's thrift's bug now. :)\")\n",
            "('12727230', 'Integrated in Cassandra #128 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/128/])\\n    default all count args to 100.  patch by jbellis for \\n')\n",
            "('12774697', 'FYI, The last link is now dead.')\n",
            "3 Key: CASSANDRA-261, Summary: get_slice needs offset + limit, Type: Bug, Status: Resolved\n",
            "Bug Resolved Normal\n",
            "Description: Right now get_slice does not allow you to provide an offset.  This would help for pagination.\n",
            "CASSANDRA-261\n",
            "Comments: 10\n",
            "('12724760', 'Me, explaining new get_slice: so now you would slice from \"\" with whatever your desired limit is, then slice from the last name that returns to page\\n\\nIan: Hm. That means that you have to chain from one call to the next, rather than being able to calculate chunksize*page.  e.g. you wouldn\\'t be able to jump from page 1 to 3.\\n\\nI\\'m convinced now -- could you submit a patch w/ your column offset code? :)')\n",
            "('12725250', 'Attach a patch that adds the offset support.\\n')\n",
            "('12725319', \"If I'm understanding this patch correctly, it will have to sequentially scan until past the offset before it can start returning results. Assuming you're using this for pagination, each call is going to require scanning through more and more results.\\n\\nIf this is the case, I'm not sure it's a good idea to hide these sorts of inefficiencies behind an API like this. \")\n",
            "('12725331', \"Yes, the code has to scan and skip the portion of data between 0 and offset. So, the larger the offset, the worse the performance. It's hard to avoid this scanning overhead since offset is based on data merged from all sstables and it's difficult to push offset into individual sstables.\\n\\nHowever, this is still better than the old get_slice that has to materialize the whole CF. A typical use case of this function is to support page scrolling. Typically, a user goes to earlier pages much more frequently. So this api could still be useful.  We should document this impact though.\\n\")\n",
            "('12726254', 'I think the argument \"I\\'m a big boy, let me make the decision whether the tradeoff is worth it\" applies here better than in CASSANDRA-262.  (It\\'s one thing to allow a client to bog down the server with something potentially slow.  It\\'s another to let the client crash you.  RDBMSes draw a similar distinction, although you could argue that they have no choice. :)')\n",
            "('12726275', \"Right. It seems that some applications need this functionality. Although this implementation doesn't give uniform performance, it is better than the old implementation or the application doing its own implementation.\")\n",
            "('12727129', 'Could you update your patch to include updating the system tests?  (setup instructions at http://wiki.apache.org/cassandra/HowToContribute)')\n",
            "('12727627', 'Patch v2.\\n* patch the system test\\n* rebase to head of trunk')\n",
            "('12727676', 'committed, thanks!')\n",
            "('12728667', 'Integrated in Cassandra #131 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/131/])\\n    add offset support to get_slice.  patch by Jun Rao; reviewed by jbellis for \\n')\n",
            "4 Key: CASSANDRA-260, Summary: Unable to read all columns in a column family from the CLI, Type: Bug, Status: Resolved\n",
            "Bug Resolved Normal\n",
            "Description: Since r788516 the typical 'get all columns for a row/cf' example no longer works.\n",
            "\n",
            "cassandra> get Table1.Standard1['jsmith']\n",
            "is the example from the wiki.\n",
            "\n",
            "This is due to Table.getSliceFrom now requiring the cfName be split by \"cf:column\", whereas the CLI is passing just \"cf\" for this Cql expression.\n",
            "\n",
            "It looks like there are two possible solutions.  One is for ColumnRangeQueryRSD to append the appropriate wildcard to columnFamily_column, i.e. cfMeta_.cfName + \":\"\n",
            "\n",
            "The other is for the Cql to be adapted to potentially expect multiple columns when it sees table.cf['rowkey'][''] or table.cf['rowkey'][':'] but I'm not sure how to do that.\n",
            "CASSANDRA-260\n",
            "Comments: 1\n",
            "('12727796', 'fixed in patch for CASSANDRA-277 (to be applied shortly, hopefully)')\n",
            "5 Key: CASSANDRA-259, Summary: LRU cache for key positions, Type: New Feature, Status: Resolved\n",
            "New Feature Resolved Normal\n",
            "Description: add cache like the old touch cache, but working :)\n",
            "\n",
            "this will mitigate the performance hit from CASSANDRA-223\n",
            "CASSANDRA-259\n",
            "Comments: 11\n",
            "('12725438', \"LinkedHashMap is a nonstarter though.  This was used in the old code but it's not threadsafe and if you wrap in the naive Collections.synchronizedMap performance will suffer since every read (from potentially lots of threads) has to go through that.\\n\\nGoing to use the one from http://code.google.com/p/concurrentlinkedhashmap/.  The race condition mentioned on the front page is fixed in trunk.\")\n",
            "('12725439', \"There's good discussion on that project and LRU cache in general in HBASE-1460 if you missed it.\")\n",
            "('12725445', 'Thanks for the pointer.  I didn\\'t actually see much about CLHM there, just the comments \"Their \\'old algorithm\\' is much like this one. The new algorithm looks very cool but is not production ready yet... Okay the algorithms i\\'m using in our version are not actually the same as their old algorithm. In any case, should stay tuned to their new algorithm.\"\\n\\nDo you think we should borrow HBase\\'s implementation instead?  I\\'m reasonably confident that CLHM is production ready given that ehcache (http://ehcache.sourceforge.net/) uses them heavily enough to have run into at least that one concurrency bug that\\'s fixed now. :)\\n')\n",
            "('12725457', \"ehcache picked up that new implementation fairly recently (release two weeks ago, from the looks of it) but it does look like they received massive benefits from it.  http://gregluck.com/blog/archives/2009/02/i_have_been_wai.html -- wow.  I didn't realize that the new design had hit major software yet, just remembered seeing jgray's comment. +1 CLHM.\")\n",
            "('12726106', '04\\n    wire up per-table cache size\\n\\n03\\n    refactor sstable into SSTable, SSTableReader, and SSTableWriter.\\n\\n02\\n    add concurrentlinkedhashmap cache and config option.  config option is NOT yet wired up to control cache sizes.\\n\\n01\\n    encapsulate bloom filter access into sstable.getPosition\\n')\n",
            "('12726249', 'Some minor comments:\\n1. Need to make it clear that the new config para keyCacheSize is in percentage of # of keys. Change it to keyCacheSizeInPCT?\\n2. Make SSTable an abstract class.\\n\\nThe bigger questions:  Do we plan to cache the column values themselves at CFS level? This seems to be the more effective caching mechanism. Will caching at CFS level obviate the need for caching key positions or do we want to support caching in multiple levels?')\n",
            "('12726253', '1. I thought the comment\\n            <!-- Key cache size is the fraction of keys per sstable whose locations\\n                 we keep in memory in \"mostly LRU\" order. -->\\n\\nmade it clear what was going on, but we can change it to KeyCachedFraction, if you like that better.  (It\\'s not a percent, since that is 0-100 not 0-1 :)\\n\\n2. done\\n\\n3: Column sizes vary a lot more than key sizes.  Also it is clear that if the client is doing a slice op on the key, we want to cache the key location, but do we want to cache the column values?  That is much less clear.  Hence I think this is best managed explicitly by the client with memcached, ehcache, etc.')\n",
            "('12726517', \"Fine. Another comment. KeyCachedFraction doesn't give an upper bound on memory consumption. Is it possible to change it to an absolute number (maybe per CF) in terms of MB?\")\n",
            "('12726772', 'Not efficiently, no.\\n\\nThe default KCF of 0.01 will use roughly the same amount of memory as the existing 1/128 key \"index\" used for binary search.  I can add a comment to that effect.')\n",
            "('12727736', 'committed with changes noted above.')\n",
            "('12728668', 'Integrated in Cassandra #131 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/131/])\\n    per-table key cache size.\\npatch by jbellis; reviewed by Jun Rao for \\nrefactor sstable into SSTable, SSTableReader, and SSTableWriter.\\npatch by jbellis; reviewed by Jun Rao for \\nadd concurrentlinkedhashmap cache and config option.  config option is NOT yet wired up to control cache sizes.\\npatch by jbellis; reviewed by Jun Rao for \\nencapsulate bloom filter access into sstable.getPosition\\npatch by jbellis; reviewed by Jun Rao for \\n')\n",
            "6 Key: CASSANDRA-258, Summary: Enhance describeTable to return Map of Column Family so that it can be programatically parsed, Type: Improvement, Status: Resolved\n",
            "Improvement Resolved Low\n",
            "Description: describeTable returns back a string which is too open and makes it hard for clients to introspect the column families of the table.\n",
            "\n",
            "The data will be returned in the following format:\n",
            "\n",
            "{ 'Standard1': { 'desc': 'Table1.Standard1(ROW_KEY, COLUMN_MAP(COLUMN_KEY, COLUMN_VALUE, COLUMN_TIMESTAMP))',\n",
            "                 'flushperiod': '60',\n",
            "                 'sort': 'Name',\n",
            "                 'type': 'Standard'},\n",
            "  'Standard2': { 'desc': 'Table1.Standard2(ROW_KEY, COLUMN_MAP(COLUMN_KEY, COLUMN_VALUE, COLUMN_TIMESTAMP))',\n",
            "                 'flushperiod': '0',\n",
            "                 'sort': 'Name',\n",
            "                 'type': 'Standard'},\n",
            "  'StandardByTime1': { 'desc': 'Table1.StandardByTime1(ROW_KEY, COLUMN_MAP(COLUMN_KEY, COLUMN_VALUE, COLUMN_TIMESTAMP))',\n",
            "                       'flushperiod': '0',\n",
            "                       'sort': 'Time',\n",
            "                       'type': 'Standard'},\n",
            "  'Super1': { 'desc': 'Table1.Super1(ROW_KEY, SUPER_COLUMN_MAP(SUPER_COLUMN_KEY, COLUMN_MAP(COLUMN_KEY, COLUMN_VALUE, COLUMN_TIMESTAMP)))',\n",
            "              'flushperiod': '0',\n",
            "              'sort': 'Name',\n",
            "              'type': 'Super'}}\n",
            "\n",
            "The CLI will also need to be updated to handle the new map that is returned\n",
            "\n",
            "CASSANDRA-258\n",
            "Comments: 3\n",
            "('12724745', 'Patch')\n",
            "('12724748', 'Excellent!  Committed.')\n",
            "('12724850', 'Integrated in Cassandra #121 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/121/])\\n    Enhance describeTable to return Map of Column Family so that it can be programatically parsed.  patch by Sammy Yu; reviewed by jbellis for \\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carrega os issues coletados em um banco (Sqlite)"
      ],
      "metadata": {
        "id": "5qfJ3cTwkg6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = IssuesDatabase(\"issues_db.db\")\n",
        "print('Popula o banco de dados issues_db...')\n",
        "total_items = len(cassandra_issues.get_issues())\n",
        "i = 0\n",
        "iterable_cassandra_issues = tqdm.tqdm(cassandra_issues.get_issues(), total=total_items)\n",
        "for each_jira_issue in iterable_cassandra_issues:\n",
        "  db.insert_in_table_issues(\"CASSANDRA\", each_jira_issue.key, each_jira_issue.summary, str(each_jira_issue.issue_type), str(each_jira_issue.status), str(each_jira_issue.priority), each_jira_issue.description)\n",
        "  if len(each_jira_issue.get_comments().keys()) > 0:\n",
        "    for issue_id,comments in each_jira_issue.get_comments().items():\n",
        "      if len(comments) > 0:\n",
        "        for comment in comments:\n",
        "          if len(comment) > 0:\n",
        "            elemento = comment[0] + ': ' + comment[1]\n",
        "            db.insert_in_table_comments(issue_id, elemento)\n",
        "  percentage = (i + 1) / total_items * 100\n",
        "  iterable_cassandra_issues.set_description(f\"Progress inserting data\")\n",
        "  i += 1\n",
        "db.close_connection()\n",
        "print('Banco issues_db carregado com sucesso!')"
      ],
      "metadata": {
        "id": "5Sn8IevEJSqU",
        "outputId": "75226f1e-cd90-44a1-d48d-7cdb6bb80ca8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress inserting data:   0%|          | 2/18579 [00:00<30:22, 10.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Popula o banco de dados issues_db...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress inserting data: 100%|ââââââââââ| 18579/18579 [28:24<00:00, 10.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Banco issues_db carregado com sucesso!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -liath"
      ],
      "metadata": {
        "id": "eh44WfSOVW92",
        "outputId": "ffca2ef0-49db-4dc4-f65f-43d1acc280c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 101M\n",
            "2752518 drwxr-xr-x 1 root root 4.0K Sep 16 19:52 .\n",
            "3145952 -rw-r--r-- 1 root root 101M Sep 16 19:52 issues_db.db\n",
            "3145950 drwxr-xr-x 2 root root 4.0K Sep 16 18:51 .ipynb_checkpoints\n",
            "3145786 -rw-r--r-- 1 root root 1.8K Sep 16 17:08 install_jira.log\n",
            "3145739 drwxr-xr-x 1 root root 4.0K Sep 16 17:07 ..\n",
            "2752519 drwxr-xr-x 1 root root 4.0K Sep 14 13:23 sample_data\n",
            "1703943 drwxr-xr-x 4 root root 4.0K Sep 14 13:22 .config\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mostra um fragmento das tabelas Issues e Comments"
      ],
      "metadata": {
        "id": "ggbGFlvBko3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = IssuesDatabase(\"issues_db.db\")\n",
        "db.show_n_lines(table='Issues', n=10)"
      ],
      "metadata": {
        "id": "VV1ixyN8jk2t",
        "outputId": "811dd93d-5f01-4182-cc87-835f09e325dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 'CASSANDRA', 'CASSANDRA-263', 'get_slice needs to support desc from last column', 'Bug', 'Resolved', 'Normal', \"At the moment there's no way to ask for a slice starting with the last column and going desc\")\n",
            "(2, 'CASSANDRA', 'CASSANDRA-262', 'get_slice needs to allow returning all columns', 'Bug', 'Resolved', 'Normal', \"Right now get_slice requires you to enter a 'large' value, -1 used to indicate 'all columns'. We should allow this.\")\n",
            "(3, 'CASSANDRA', 'CASSANDRA-261', 'get_slice needs offset + limit', 'Bug', 'Resolved', 'Normal', 'Right now get_slice does not allow you to provide an offset.  This would help for pagination.')\n",
            "(4, 'CASSANDRA', 'CASSANDRA-260', 'Unable to read all columns in a column family from the CLI', 'Bug', 'Resolved', 'Normal', 'Since r788516 the typical \\'get all columns for a row/cf\\' example no longer works.\\n\\ncassandra> get Table1.Standard1[\\'jsmith\\']\\nis the example from the wiki.\\n\\nThis is due to Table.getSliceFrom now requiring the cfName be split by \"cf:column\", whereas the CLI is passing just \"cf\" for this Cql expression.\\n\\nIt looks like there are two possible solutions.  One is for ColumnRangeQueryRSD to append the appropriate wildcard to columnFamily_column, i.e. cfMeta_.cfName + \":\"\\n\\nThe other is for the Cql to be adapted to potentially expect multiple columns when it sees table.cf[\\'rowkey\\'][\\'\\'] or table.cf[\\'rowkey\\'][\\':\\'] but I\\'m not sure how to do that.')\n",
            "(5, 'CASSANDRA', 'CASSANDRA-259', 'LRU cache for key positions', 'New Feature', 'Resolved', 'Normal', 'add cache like the old touch cache, but working :)\\n\\nthis will mitigate the performance hit from CASSANDRA-223')\n",
            "(6, 'CASSANDRA', 'CASSANDRA-258', 'Enhance describeTable to return Map of Column Family so that it can be programatically parsed', 'Improvement', 'Resolved', 'Low', \"describeTable returns back a string which is too open and makes it hard for clients to introspect the column families of the table.\\n\\nThe data will be returned in the following format:\\n\\n{ 'Standard1': { 'desc': 'Table1.Standard1(ROW_KEY, COLUMN_MAP(COLUMN_KEY, COLUMN_VALUE, COLUMN_TIMESTAMP))',\\n                 'flushperiod': '60',\\n                 'sort': 'Name',\\n                 'type': 'Standard'},\\n  'Standard2': { 'desc': 'Table1.Standard2(ROW_KEY, COLUMN_MAP(COLUMN_KEY, COLUMN_VALUE, COLUMN_TIMESTAMP))',\\n                 'flushperiod': '0',\\n                 'sort': 'Name',\\n                 'type': 'Standard'},\\n  'StandardByTime1': { 'desc': 'Table1.StandardByTime1(ROW_KEY, COLUMN_MAP(COLUMN_KEY, COLUMN_VALUE, COLUMN_TIMESTAMP))',\\n                       'flushperiod': '0',\\n                       'sort': 'Time',\\n                       'type': 'Standard'},\\n  'Super1': { 'desc': 'Table1.Super1(ROW_KEY, SUPER_COLUMN_MAP(SUPER_COLUMN_KEY, COLUMN_MAP(COLUMN_KEY, COLUMN_VALUE, COLUMN_TIMESTAMP)))',\\n              'flushperiod': '0',\\n              'sort': 'Name',\\n              'type': 'Super'}}\\n\\nThe CLI will also need to be updated to handle the new map that is returned\\n\")\n",
            "(7, 'CASSANDRA', 'CASSANDRA-257', 'Eliminate thrift warnings ', 'Improvement', 'Resolved', 'Low', 'Running \"thrift --gen py cassandra.thrift\" produces a bunch of warnings.\\nThese go away when there\\'s a field key for the params in the interface definitions.\\nThis is fixed in trunk, but seems to have escaped 0.3.\\n')\n",
            "(8, 'CASSANDRA', 'CASSANDRA-256', 'Improve ListenAddress in Storage-Conf', 'Improvement', 'Resolved', 'Low', \"Right now in storage-conf we have:\\n\\n<ListenAddress>localhost</ListenAddress>\\n\\nWe should probably change this to 0.0.0.0 by default for new comers, since setting up multi-node installation with localhost, isn't going to work right. Right now 0.0.0.0 might not work right, (leaving blank does). So we should check that in DatabaseDescriptor. Also a nice to have is maybe an XML doc comment.\")\n",
            "(9, 'CASSANDRA', 'CASSANDRA-255', 'Supercolumn deserialization bug', 'Bug', 'Resolved', 'Normal', None)\n",
            "(10, 'CASSANDRA', 'CASSANDRA-254', 'clean up sstable constructors', 'Improvement', 'Resolved', 'Normal', None)\n",
            "(11, 'CASSANDRA', 'CASSANDRA-253', 'add capability to query CFS attributes to nodeprobe', 'New Feature', 'Resolved', 'Low', 'There is a lot of interesting CF-specific instrumentation provided by the CFS Mbeans. It would be nice if these attributes/operations where available through the nodeprobe utility.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db.show_n_lines(table='Comments', n=10)"
      ],
      "metadata": {
        "id": "ok88vbZqjqpN",
        "outputId": "27b5e0e4-88f6-4abb-9236-80daf786a694",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 'CASSANDRA-263', \"12724762: Is the problem that you don't know what the last column is? Normally, an application should be able to insert a fake column that's always larger than any real columns. Do you think that's good enough?\")\n",
            "(2, 'CASSANDRA-263', '12724766: Yes.\\n\\nI was thinking that we could have the \"CF\" format (w/ no \":\") stand for \"start w/ first if asc or last if desc.\"  Reading the last from the column index should be reasonably efficient, right?')\n",
            "(3, 'CASSANDRA-263', '12725251: Using the column index to start with the last column group is possible and is efficient. I am just wondering if it is a good idea to give empty starting column different meanings depending on the ordering.')\n",
            "(4, 'CASSANDRA-263', \"12725254: I'm open to alternative APIs, but that one makes sense to me and doesn't require adding extra parameters :)\")\n",
            "(5, 'CASSANDRA-263', \"12737354: Attach a fix. If startColumn is empty and isAscending is false, assume that we scan from the largest column in descending order.\\n\\nAlso fix a bug in SliceFilterQuery.filterSuperColumn() that didn't handle descending order properly.\\n\")\n",
            "(6, 'CASSANDRA-263', '12737550: committed, thanks!')\n",
            "(7, 'CASSANDRA-263', '12737872: Integrated in Cassandra #154 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/154/])\\n    allow start of [] to mean \"start with the largest value\" when ascending=false.  patch by Jun Rao; reviewed by jbellis for \\n')\n",
            "(8, 'CASSANDRA-262', '12726114: How about if we add a configuration parameter for \"maximum columns to slice at once,\" that defaults to something high enough (1000?) that you really ought to page if you need more than that anyway, and low enough that it\\'s in no danger of crashing your server from OOM?')\n",
            "(9, 'CASSANDRA-262', '12726117: +1. I am okay with this.')\n",
            "(10, 'CASSANDRA-262', \"12726121: Eric, since you're already in the slice code for CASSANDRA-263, do you think you could take a stab at this too?\")\n",
            "(11, 'CASSANDRA-262', '12726248: Actually, as with many of my \"brilliant\" ideas, this one looks worse given a little more time. :)\\n\\nI don\\'t see what purpose this would serve over the client just sending 1000 for the count, other than obfuscation.')\n"
          ]
        }
      ]
    }
  ]
}